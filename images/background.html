<!DOCTYPE html>
<!-- saved from url=(0042)http://localhost:4000/#Call%20for%20Papers -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>ENLSP NeurIPS Workshop 2021 | ENLSP highlights some fundamental problems in NLP and speech processing related to efficiency of the models, training and inference for the general ML and DL communities.</title>
<meta name="generator" content="Jekyll v4.2.0">
<meta property="og:title" content="ENLSP NeurIPS Workshop 2021">
<meta name="author" content="ENLSP NeurIPS Workshop 2021">
<meta property="og:locale" content="en_US">
<meta name="description" content="ENLSP highlights some fundamental problems in NLP and speech processing related to efficiency of the models, training and inference for the general ML and DL communities.">
<meta property="og:description" content="ENLSP highlights some fundamental problems in NLP and speech processing related to efficiency of the models, training and inference for the general ML and DL communities.">
<meta property="og:site_name" content="ENLSP NeurIPS Workshop 2021">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="ENLSP NeurIPS Workshop 2021">
<script type="application/ld+json">
{"headline":"ENLSP NeurIPS Workshop 2021","@type":"WebSite","url":"/","author":{"@type":"Person","name":"ENLSP NeurIPS Workshop 2021"},"name":"ENLSP NeurIPS Workshop 2021","description":"ENLSP highlights some fundamental problems in NLP and speech processing related to efficiency of the models, training and inference for the general ML and DL communities.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="./background_files/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="ENLSP NeurIPS Workshop 2021">
</head>
<body><link rel="preconnect" href="https://fonts.gstatic.com/">
<link href="./background_files/css2" rel="stylesheet">
<style>
.sample-header {
  left: 0;
  top: 0;
  width: 100%;
  background-image: url("https://cdn1.matadornetwork.com/blogs/1/2011/05/Sydney-Australia-cityscape-destination-940x627.jpg");
  background-position: center;
  background-size: cover;
  background-repeat: no-repeat;
}
.sample-header::before {
  position: absolute;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  //background-color: MidnightBlue;
  opacity: 0.3;
}
.sample-header-section {
  position: relative;
  padding: 15% 0 10%;
  max-width: 640px;
  margin-left: auto;
  margin-right: auto;
  color: white;
  text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.5);
  font-family: "Roboto Condensed", sans-serif;
}
#big_name {
  font-weight: 500;
}
#small_name {
  font-weight: 400;
  padding-bottom:10%;
}

</style>

<header class="site-header">
  <div class="wrapper"><!--<a class="site-title" rel="author" href="/">ENLSP NeurIPS Workshop 2021</a>-->
	
	
	<div class="sample-header">
	  <div class="sample-header-section">
		<h2 id="big_name">ENLSP NeurIPS Workshop 2021</h2>
		<h1 id="small_name">Efficient Natural Language <br> and Speech Processing</h1>
	  </div>
	</div><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <!-- Adding customized header (Anderson) -->
        <div class="trigger"><a class="page-link" href="http://localhost:4000/#Overview"> Overview </a><a class="page-link" href="http://localhost:4000/#Call%20for%20Papers"> Call for Papers </a><a class="page-link" href="http://localhost:4000/#Organizers"> Organizers </a><a class="page-link" href="http://localhost:4000/#Confirmed%20Spearkers"> Confirmed Spearkers </a><a class="page-link" href="http://localhost:4000/#Technical%20Committee"> Technical Committee </a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><!-- <img src="/images/deep.jpg"> -->
<center>
<h2 class="blackpar_title">Efficient Natural Language and Speech Processing </h2>
<h3 class="blackpar_title">(Models, Training and Inference)</h3>
</center>
<p>

This workshop aims at introducing some fundamental problems in the field of natural language and speech processing which can be of interest to the general machine learning and deep learning community to improve the efficiency of the models, their training and inference. The workshop program offers an interactive platform for gathering experts and talents from academia and industry through different invited keynote talks, panel discussions, paper submissions and reviews, poster and oral presentations and a mentorship program.
This will provide an opportunity to discuss and learn from each other, exchange ideas, build connections, and brainstorm on potential solutions and future collaborations. The topics of this workshop can be of interest for people working on general machine learning, deep learning, optimization, theory and NLP &amp; Speech applications.

<!--	
The workshop will take place on <span class="blackhighlighted">DATE, 2021</span>. 
Due to the pandemic, the workshop will be <span class="blackhighlighted">VIRTUAL</span>. More details will be provided soon. 

Note that to attend the event, a registration on the ICLR website is required. All workshop events (except Poster session and open discussion) can be followed using the ICLR link or use the zoom link by clicking on “join zoom” on the ICLR link. For the Poster session participants should une the Gather.town link. Note that papers id can be found on Accepter papers section.
</p>
<br>
-->
<br><br>
</p><h2 class="blackpar_title" id="Overview">Overview</h2>
<p>
Despite the great success of deep neural networks due to huge over-parameterization and using very large amount of data in different tasks of natural language processing (NLP) and speech processing, training or deploying these networks on devices or even cloud services with limited memory and computational power can be very expensive and challenging. For instance, pre-trained language models (PLMs) such as GPT-3 have led to a great breakthrough in NLP; but running GPT-3 with more than 170 billion parameters trained with more than 500 GB of data requires more than 10 Tesla V-100 GPUs. That being said, still improving the NLP and Speech models by increasing their number of parameters and incorporating more data is deemed a very common practice in the NLP and Speech domains. Therefore, it is of vital importance to invest on enhancing the efficiency of these models in terms of model architectures, training and inference from different perspectives highlighted in this workshop. In this regard, we would like to share some unique and fundamental challenges with the NeurIPS community to be considered in their future investigations.
</p>
<br><br>

<!-- Call for Papers -->
<h2 class="blackpar_title" id="Call for Papers">Call for Papers</h2>



We encourage the NeurIPS community to submit their solutions, ideas, and ongoing work concerning data, model, training, and inference efficiency for NLP and speech processing. The scope of this workshop includes, but not limited to, the following topics.

<br><br>
<b>Efficient Pre-Training and Fine-Tuning.</b> Pre-training is a very expensive process. Even a small modification to the configuration of the models requires the user to redo pre-training:
<br>

<ul>
	<li>Fast pre-training techniques, avoiding pre-training from scratch</li>
	<li>Multi-domain pre-training/fine-tuning and fast domain adaptation for pre-trained/fine tuned models</li>
	<li>Multimodal pre-trained (e.g., text--speech) models</li>
	<li>Avoiding task-specific fune-tuning of pre-trained models</li>
	<li>New efficient architectures for pre-trained models</li>
</ul>

<br>
<b>Model Compression.</b> Neural model compression techniques such as quantization, pruning, layer decomposition and knowledge distillation (KD) aim at reducing the number of parameters of the models, improving their memory requirements or running efficiency:
<br>

<ul>
	<li>Impact of different compression techniques on the inductive biases learned by the original models</li>
	<li>Combined compression techniques for more efficient NLP and speech models</li>
	<li>Efficient KD for NLP and speech, efficient intermediate layer distillation, and teacher-free distillation</li>
	<li>Improving KD for large classification problems (e.g., text generation and machine translation with a very large number of output classes)</li>
	<li>Solving the <i>Capacity Gap</i> problem and the <i>Search Problem</i> associated with finding the best checkpoint of the teacher</li>
	<li>Theory of KD (e.g., how does KD work?) </li>
</ul>

<br>
<b>Efficient Training.</b> How to improve the training speed of the NLP and speech models:
<br>
<ul>
	<li>Improving the optimizer for faster training</li>
	<li>Accelerated training of different tasks in NLP and speech</li>
	<li>Distributed training,  federated learning and continual learning for NLP and speech tasks </li>
</ul>

<br>
<b>Data Efficiency.</b> Pre-trained models rely on a huge amount of unlabeled data which makes the training very sample inefficient:
<br>
<ul>
	<li>Sample efficient training, training with less data, few-shot and zero-shot learning</li>
	<li>Sample efficient data-augmentation, identifying which training samples should be augmented</li>
	<li>Low-resource NLP and speech, considering training tasks with limited available data</li>
</ul>

<br>
<b>Edge Intelligence.</b>  Running the trained models on edge devices will require a conversion process to match the network with hardware specifications:
<br>
<ul>
	<li>TinyML for NLP and speech on embedded systems</li>
	<li>Efficient conversion versus hardware-aware training</li>
	<li>Training on device</li>
</ul>


<h2 class="blackpar_title">Important Dates:</h2>
<ul>
	<li>Submission Deadline: September 17, 2021</li>
	<li>Acceptance Notification: October 22, 2021</li>
	<li>Camera-Ready Submission: November 1, 2021</li>
	<li>Workshop Date: December 13, 2021</li>
</ul>

<br><br>
<!--Confirmed Spearkers-->
<h2 class="blackpar_title" id="Confirmed Spearkers">Confirmed Spearkers</h2>
<div class="row">
	<div class="card column">
	  <img src="./background_files/mirella-lapata.jpeg" alt="Mirella Lapata" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Prof.<br>Mirella Lapata</b>
			<br>
			University of Edinburgh
		</h4>
		</center>
	  </div>
	</div>
	<div class="card column">
	  <img src="./background_files/zettlemoyer.jpg" alt="Luke Zettlemoyer" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Prof.<br>Luke Zettlemoyer</b>
			<br>
			University of Washington
		</h4>
		</center>
	  </div>
	</div>
	<div class="card column">
	  <img src="./background_files/kevin.jpg" alt="Kevin Duh" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Prof.<br>Kevin Duh</b>
			<br>
			Johns Hopkins University
		</h4>
		</center>
	  </div>
	</div>
	<div class="card column">
	  <img src="./background_files/boxing.jpg" alt="Boxing Chen" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Dr.<br>Boxing Chen</b>
			<br>
			Alibaba
		</h4>
		</center>
	  </div>
	</div>
</div>
<div class="row">
	<div class="card column">
	  <img src="./background_files/sameer_singh.jpg" alt="Saneer Singh" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Prof.<br>Sameer Singh</b>
			<br>
			University of California
		</h4>
		</center>
	  </div>
	</div>
	<div class="card column">
	  <img src="./background_files/danqi_2019.jpg" alt="Danqi Chen" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Prof.<br>Danqi Chen</b>
			<br>
			Assistant professor, Princeton University
		</h4>
		</center>
	  </div>
	</div>	
	<div class="card column">
	  <img src="./background_files/norouzi.jpg" alt="Mohammad Norouzi" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Dr.<br>Mohammad Norouzi</b>
			<br>
			Google Brain
		</h4>
		</center>
	  </div>
	</div>
	<div class="card column">
	  <img src="./background_files/Yejin.jpg" alt="Yejin Choi" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Prof.<br>Yejin Choi</b>
			<br>
			University of Washington
		</h4>
		</center>
	  </div>
	</div>
</div>
<div class="row">
	<div class="card column" id="to_right">
	  <img src="./background_files/xinjiang.jpg" alt="Xin Jiang" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Dr.<br>Xin Jiang</b>
			<br>
			Huawei
		</h4>
		</center>
	  </div>
	</div>

	<div class="card column">
	  <img src="./background_files/xu_sun.jpg" alt="Xu Sun" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Prof.<br>Xu Sun</b>
			<br>
			Peking University
		</h4>
		</center>
	  </div>
	</div>	
	

	<div class="card column">
	  <img src="./background_files/barbara.png" alt="Barbara Plank" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Prof.<br>Barbara Plank</b>
			<br>
			IT University of Copenhagen
		</h4>
		</center>
	  </div>
	</div>
</div>
<br> <br>
<!-- Organizers -->
<h2 class="blackpar_title" id="Organizers">Organizers</h2>
<div class="row">
	<div class="card column" style="margin-left:13%;">
	  <img src="./background_files/Mehdi_Rezagholizadeh.jpg" alt="Mehdi Rezagholizadeh" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Mehdi Rezagholizadeh</b>
			<br>
			Huawei Noah's Ark Lab
		</h4>
		</center>
	  </div>
	</div>
	<div class="card column">
	  <img src="./background_files/lili_mou.jpg" alt="Lili Mou" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Lili Mou</b>
			<br>
			University of Alberta
		</h4>
		</center>
	  </div>
	</div>
	<div class="card column">
	  <img src="./background_files/Yue_Dong.jpg" alt="Yue Dong" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Yue Dong</b>
			<br>
			McGill University &amp; MILA
		</h4>
		</center>
	  </div>
	</div>

</div>
<div class="row">
	<div class="card column" style="margin-left:13%;">
	  <img src="./background_files/pascal_poupart.jpg" alt="Pascal Poupart" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Pascal Poupart</b>
			<br>
			University of Waterloo
		</h4>
		</center>
	  </div>
	</div>

	<div class="card column">
	  <img src="./background_files/ali_ghodsi.jpg" alt="Ali Ghodsi" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Ali Ghodsi</b>
			<br>
			University of Waterloo
		</h4>
		</center>
	  </div>
	</div>
	<div class="card column">
	  <img src="./background_files/qun_liu.png" alt="Qun Liu" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<b>Qun Liu</b>
			<br>
			Huawei Noah's Ark Lab
		</h4>
		</center>
	  </div>
	</div>
</div>

<br><br>
<!-- Technical Committee -->
<h2 class="blackpar_title" id="Technical Committee">Technical Committee</h2>

<ul>
	<li>Kevin Duh (Johns Hopkins University)</li>
	<li>Bang Liu (University of Montreal (UDM))</li>
	<li>Wulong Liu (Huawei Noah's Ark Lab)</li>
	<li>Peyman Passban (Amazon)</li>
	<li>Ivan Kobyzev (Huawei Noah's Ark Lab)</li>
	<li>Jad Kabbara (McGill University &amp; MILA)</li> 
	<li>Aref Jafari (University of Waterloo)</li> 
	<li>Ahmad Rashid (Huawei Noah's Ark Lab)</li> 
	<li>Shailza Jolly (TU Kaiserslautern)</li> 
	<li>Md. Akmal Haidar (Huawei Noah's Ark Lab)</li> 
	<li>Jingjing Xu (ByteDance)</li> 
	<li>Vasileios Lioutas (University of British Colombia (UBC))</li> 
	<li>Anderson R. Avila (Huawei Noah's Ark Lab)</li> 
	<li>Malik H. Altakrori (McGill University &amp; MILA)</li> 
	<li>Ali Vahdat (Thomson Reuters)</li> 
	<li>Prasanna Parthasarathi (McGill University &amp; MILA)</li> 
	<li>Fattane Zarrinkalam (Thomson Reuters)</li> 
	<li>Makesh S Narsimhan (McGill University &amp; MILA)</li> 
	<li>Nasrin Taghizadeh (University of Tehran)</li> 
	<li>Borna Jafarpour (Thomson Reuters)</li> 
	<li>Shohreh Shaghaghian (Thomson Reuters)</li> 
	<li>Ehsan Kamalloo (University of Alberta)</li>
	<li>Ali Saheb Pasand (University of Waterloo)</li>
	<li>Abbas Ghaddar (Huawei Noah's Ark Lab)</li>
	<li>Mehrdad Ganjeh (Ernst &amp; Young (EY))</li>
	<li>Mingxuan Wang (ByteDance)</li>
</ul>
<p></p>



  </div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <ul class="contact-list">
          <li class="p-name">ENLSP NeurIPS Workshop 2021</li>
          <li><a class="u-email" href="mailto:neurips.ENLSP.2021@gmail.com">neurips.ENLSP.2021@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>ENLSP highlights some fundamental problems in NLP and speech  processing related to efficiency of the models, training and  inference for the general ML and DL communities.
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>

<!-- Sponsors -->
<!-- <h2 class="par_title" id="sponsors">Our sponsors</h2> -->
<!--
<div class="row">
  <img class="column" src="/images/huawei.jpg">
    <img class="column" src="/images/darwin_ai.jpg">
  <img class="column" src="/images/huawei.jpg">
    <img class="column" src="/images/darwin_ai.jpg">
</div>--></div>

  </div>

</footer>



</body></html>